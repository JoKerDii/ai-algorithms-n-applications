For each point, cite the sentence written by the author that best states the point.

Sure, here are the citations for each point mentioned in the article:

1. **Importance of RLHF:**
   - "RLHF is an integral part of the modern LLM training pipeline due to its ability to incorporate human preferences into the optimization landscape, which can improve the model's helpfulness and safety."

2. **Overview of the Canonical LLM Training Pipeline:**
   - "Modern transformer-based LLMs, such as ChatGPT or Llama 2, undergo a 3-step training procedure: 1. Pretraining 2. Supervised finetuning 3. Alignment"

3. **Pretraining Phase:**
   - "Pretraining typically occurs on a vast text corpus comprising billions to trillions of tokens."

4. **Supervised Finetuning Phase:**
   - "The supervised finetuning stage involves another round of next-token prediction. However, unlike the preceding pretraining stage, we now work with instruction-output pairs."

5. **Alignment Phase:**
   - "Following this supervised finetuning stage, there's another finetuning phase commonly regarded as the 'alignment' step, as its primary objective is to align the LLM with human preferences."

6. **RLHF Procedure:**
   - "The RLHF pipeline takes a pretrained model and finetunes it in a supervised fashion (step 2 in the previous section) and further aligns it with proximal policy optimization (step 3 in the previous section)."

7. **RLHF in Llama 2:**
   - "Meta AI utilized RLHF in creating the Llama-2-chat models as well. Nevertheless, there are several distinctions between the two approaches, which I've highlighted in the annotated figure below."

8. **Margin Loss in Llama 2:**
   - "Moreover, whatâ€™s new is that alongside each binary rank, a 'margin' label (ranging from 'significantly better' to 'negligibly better') is gathered, which can optionally be used in the binary ranking loss via an additional margin parameter to calculate the gap between the two responses."

9. **Two Reward Models in Llama 2:**
   - "As mentioned earlier, there are two reward models in Llama 2 instead of one. One reward model is based on helpfulness, and the other is based on safety."

10. **Rejection Sampling in Llama 2:**
    - "Instead of solely relying on the RLHF with PPO method we discussed earlier, they employ two algorithms for RLHF finetuning: PPO and rejection sampling."

11. **RLHF Alternatives:**
    - "However, a lot of ongoing research focuses on developing more efficient alternatives. The most interesting approaches are summarized below."

12. **Constitutional AI:**
    - "In this Constitutional AI paper, researchers propose a self-training mechanism based on a list of rules humans provide."

13. **Wisdom of Hindsight:**
    - "The Wisdom of Hindsight makes Language Models Better Instruction Followers shows that supervised approaches to LLM finetuning can indeed work well."

14. **Direct Preference Optimization:**
    - "Direct Preference Optimization (DPO) is an alternative to RLHF with PPO where the researchers show that the cross entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM."

15. **Contrastive Preference Learning:**
    - "Like Direct Preference Optimization (DPO), Contrastive Preference Learning (CPL) is an approach to simplify RLHF by eliminating the reward model learning."

16. **Reinforced Self-Training (ReST):**
    - "ReST is an alternative to reinforcement learning with human feedback (RLHF) that aligns LLMs with human preferences."

17. **RLAIF:**
    - "The recent reinforcement learning with AI feedback (RLAIF) study shows that the ratings for the reward model training in RLHF don't necessarily have to be provided by humans but can be generated by an LLM (here: PaLM 2)."

18. **Final Remarks:**
    - "Whether these alternatives will be worthwhile in practice remains to be seen as there is currently no true competitor to Llama 2 and Code Llama-scale models that have been trained without RLHF."